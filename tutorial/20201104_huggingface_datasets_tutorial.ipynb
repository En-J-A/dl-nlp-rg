{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface_datasets_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1a67fa6718b4a12890b040ab9c841fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6835da877c93409fb85cd6107f1405c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77f14e964fb54bb6a0d61ab867809838",
              "IPY_MODEL_61e4b60466374ef8a9d3243272ed4e7d"
            ]
          }
        },
        "6835da877c93409fb85cd6107f1405c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77f14e964fb54bb6a0d61ab867809838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8d882d2bf97949cfa3dc67cc3d840470",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 380,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 380,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea8e5ba248224bc0aa9399d86ec6fd65"
          }
        },
        "61e4b60466374ef8a9d3243272ed4e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_388408a8f5fa4151b9d0676e5a17625e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 380/380 [00:22&lt;00:00, 16.78ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9bf093a92404906b6268a7331fdc182"
          }
        },
        "8d882d2bf97949cfa3dc67cc3d840470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea8e5ba248224bc0aa9399d86ec6fd65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "388408a8f5fa4151b9d0676e5a17625e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9bf093a92404906b6268a7331fdc182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0KPLh7hQ9i"
      },
      "source": [
        "## Hugging Face datasets library\n",
        "\n",
        "### Goal of this tutorial:\n",
        "- Know the usage of datasets library from Hugging Face\n",
        "- What will be covered:\n",
        "    - Quick overview\n",
        "    - Installation\n",
        "    - Datasets\n",
        "        - List available datasets\n",
        "        - Online dataset explorer \n",
        "        - Loading yelp dataset\n",
        "        - Inspect and modify yelp dataset\n",
        "        - Train a BERT model using yelp dataset\n",
        "    - Metrics\n",
        "        - List available metrics\n",
        "        - Load NER evaluation metric\n",
        "    - Loading custom dataset\n",
        "        - Loading from a python dictionary\n",
        "        - Loading from a pandas dataframe\n",
        "        - Using a custom dataset loading script (Bonus)\n",
        "        \n",
        "\n",
        "###  General:\n",
        "- This notebook was last tested on Python 3.6.4, PyTorch 1.4.0, transformers 2.0.0, datasets 1.1.2 \n",
        "- We would like to acknowledge the tutorial on datasets library from huggingface (https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) which we used as a reference.\n",
        "\n",
        "\n",
        "### References:\n",
        "To know more about the above-mentioned concepts, take a look at the following:\n",
        "1. Original GitHub repository (https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb)\n",
        "2. Documentation (https://huggingface.co/docs/datasets/)\n",
        "3. Online dataset explorer (https://huggingface.co/nlp/viewer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdDrBTh6hQ9j"
      },
      "source": [
        "## Quick overview\n",
        "\n",
        "`ðŸ¤—Datasets` is a fast and efficient library to easily share and load dataset and evaluation metrics, already providing access to 150+ datasets and 12+ evaluation metrics.\n",
        "\n",
        "The library has several interesting features (besides easy access to datasets/metrics):\n",
        "\n",
        "- Built-in interoperability with PyTorch, Tensorflow 2, Pandas and Numpy\n",
        "- Lighweight and fast library with a transparent and pythonic API\n",
        "- Strive on large datasets: frees you from RAM memory limits, all datasets are memory-mapped on drive by default.\n",
        "- Smart caching with an intelligent `tf.data`-like cache: never wait for your data to process several times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptuE_SWAhQ9k"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Let's install the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-8sTJJuhQ9l",
        "outputId": "022e0af4-5508-4992-a98d-7ab1b9a385dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install datasets\n",
        "\n",
        "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
        "import pyarrow\n",
        "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fbiXhOShQ9p"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Let's import the library. We typically only need at most four methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOCESEkrhQ9q"
      },
      "source": [
        "from datasets import list_datasets, list_metrics, load_dataset, load_metric\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta67SCu1hQ9v"
      },
      "source": [
        "### List available datasets\n",
        "Let's list the currently available datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywdc_owohQ9v",
        "outputId": "d6d0f7f6-2e0b-4ada-a937-7aef7a78625c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "datasets = list_datasets()\n",
        "print(f\"Currently {len(datasets)} datasets are available on the hub:\")\n",
        "pprint(datasets, compact=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently 170 datasets are available on the hub:\n",
            "['aeslc', 'ag_news', 'ai2_arc', 'allocine', 'amazon_us_reviews', 'anli', 'arcd',\n",
            " 'art', 'aslg_pc12', 'billsum', 'biomrc', 'blended_skill_talk', 'blimp',\n",
            " 'blog_authorship_corpus', 'bookcorpus', 'boolq', 'break_data', 'c4', 'cfq',\n",
            " 'civil_comments', 'clue', 'cmrc2018', 'cnn_dailymail', 'coarse_discourse',\n",
            " 'com_qa', 'common_gen', 'commonsense_qa', 'compguesswhat', 'conll2000',\n",
            " 'conll2003', 'coqa', 'cornell_movie_dialog', 'cos_e', 'cosmos_qa', 'crd3',\n",
            " 'crime_and_punish', 'csv', 'daily_dialog', 'definite_pronoun_resolution',\n",
            " 'discofuse', 'docred', 'doqa', 'drop', 'eli5', 'emo', 'emotion',\n",
            " 'empathetic_dialogues', 'eraser_multi_rc', 'esnli', 'event2Mind', 'fever',\n",
            " 'flores', 'fquad', 'gap', 'germeval_14', 'gigaword', 'glue',\n",
            " 'guardian_authorship', 'hans', 'hansards', 'hellaswag', 'hotpot_qa',\n",
            " 'hyperpartisan_news_detection', 'imdb', 'iwslt2017', 'jeopardy', 'json',\n",
            " 'kilt_tasks', 'kilt_wikipedia', 'kor_nli', 'lc_quad', 'lhoestq/squad',\n",
            " 'librispeech_lm', 'lince', 'lm1b', 'math_dataset', 'math_qa', 'matinf', 'mlqa',\n",
            " 'mlsum', 'movie_rationales', 'ms_marco', 'multi_news', 'multi_nli',\n",
            " 'multi_nli_mismatch', 'mwsc', 'natural_questions', 'newsgroup', 'newsroom',\n",
            " 'openbookqa', 'openwebtext', 'opinosis', 'pandas', 'para_crawl', 'pg19',\n",
            " 'piaf', 'polyglot_ner', 'qa4mre', 'qa_zre', 'qangaroo', 'qanta', 'qasc',\n",
            " 'quail', 'quarel', 'quartz', 'quora', 'quoref', 'race', 'reclor', 'reddit',\n",
            " 'reddit_tifu', 'reuters21578', 'rotten_tomatoes', 'scan', 'scicite',\n",
            " 'scientific_papers', 'scifact', 'sciq', 'scitail', 'search_qa', 'sentiment140',\n",
            " 'snli', 'social_bias_frames', 'social_i_qa', 'sogou_news', 'squad', 'squad_es',\n",
            " 'squad_it', 'squad_v1_pt', 'squad_v2', 'squadshifts',\n",
            " 'sshleifer/pseudo_bart_xsum', 'style_change_detection', 'super_glue',\n",
            " 'ted_hrlr', 'ted_multi', 'text', 'tiny_shakespeare', 'trec', 'trivia_qa',\n",
            " 'tydiqa', 'ubuntu_dialogs_corpus', 'web_of_science', 'web_questions',\n",
            " 'wiki40b', 'wiki_dpr', 'wiki_qa', 'wiki_snippets', 'wiki_split', 'wikihow',\n",
            " 'wikipedia', 'wikisql', 'wikitext', 'winogrande', 'wiqa', 'wmt14', 'wmt15',\n",
            " 'wmt16', 'wmt17', 'wmt18', 'wmt19', 'wmt_t2t', 'wnut_17', 'x_stance', 'xcopa',\n",
            " 'xnli', 'xquad', 'xsum', 'xtreme', 'yelp_polarity']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp9ea-vshQ90"
      },
      "source": [
        "### Online dataset viewer\n",
        "\n",
        "All these datasets can also be browsed on \n",
        "* the HuggingFace Hub (https://huggingface.co/datasets) \n",
        "* the ðŸ¤—datasets viewer (https://huggingface.co/nlp/viewer/)\n",
        "\n",
        "For the sake of understanding we will look at yelp reviews polarity dataset.\n",
        "\n",
        "#### Task\n",
        "Binary Sentiment Classification: Given a yelp review, the task is to predict the sentiment for the given review.\n",
        "\n",
        "#### Dataset\n",
        "The yelp reviews polarity dataset is constructed by considering stars 1 and 2 negative, and 3 and 4 positive. For each polarity 280,000 training samples and 19,000 testing samples are take randomly. In total there are 560,000 training samples and 38,000 testing samples. Negative polarity is class 1, and positive class 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYJEs2WXhQ90"
      },
      "source": [
        "### Loading yelp dataset\n",
        "\n",
        "Before downloading any dataset, we can access various attributes of the datasets. We will access the attributes of yelp dataset now: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVjIR5ghhQ91",
        "outputId": "d17388fe-3204-4df8-ef48-7b7129cec2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "yelp_dataset = list_datasets(with_details=True)[datasets.index('yelp_polarity')]\n",
        "pprint(yelp_dataset.__dict__)  # It's a simple python dataclass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'author': None,\n",
            " 'citation': '@article{zhangCharacterlevelConvolutionalNetworks2015,\\n'\n",
            "             '  archivePrefix = {arXiv},\\n'\n",
            "             '  eprinttype = {arxiv},\\n'\n",
            "             '  eprint = {1509.01626},\\n'\n",
            "             '  primaryClass = {cs},\\n'\n",
            "             '  title = {Character-Level {{Convolutional Networks}} for {{Text '\n",
            "             'Classification}}},\\n'\n",
            "             '  abstract = {This article offers an empirical exploration on '\n",
            "             'the use of character-level convolutional networks (ConvNets) for '\n",
            "             'text classification. We constructed several large-scale datasets '\n",
            "             'to show that character-level convolutional networks could '\n",
            "             'achieve state-of-the-art or competitive results. Comparisons are '\n",
            "             'offered against traditional models such as bag of words, n-grams '\n",
            "             'and their TFIDF variants, and deep learning models such as '\n",
            "             'word-based ConvNets and recurrent neural networks.},\\n'\n",
            "             '  journal = {arXiv:1509.01626 [cs]},\\n'\n",
            "             '  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n'\n",
            "             '  month = sep,\\n'\n",
            "             '  year = {2015},\\n'\n",
            "             '}',\n",
            " 'description': 'Large Yelp Review Dataset.\\n'\n",
            "                'This is a dataset for binary sentiment classification. We '\n",
            "                'provide a set of 560,000 highly polar yelp reviews for '\n",
            "                'training, and 38,000 for testing. \\n'\n",
            "                'ORIGIN\\n'\n",
            "                'The Yelp reviews dataset consists of reviews from Yelp. It is '\n",
            "                'extracted\\n'\n",
            "                'from the Yelp Dataset Challenge 2015 data. For more '\n",
            "                'information, please\\n'\n",
            "                'refer to http://www.yelp.com/dataset_challenge\\n'\n",
            "                '\\n'\n",
            "                'The Yelp reviews polarity dataset is constructed by\\n'\n",
            "                'Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\n'\n",
            "                'It is first used as a text classification benchmark in the '\n",
            "                'following paper:\\n'\n",
            "                'Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level '\n",
            "                'Convolutional Networks\\n'\n",
            "                'for Text Classification. Advances in Neural Information '\n",
            "                'Processing Systems 28\\n'\n",
            "                '(NIPS 2015).\\n'\n",
            "                '\\n'\n",
            "                '\\n'\n",
            "                'DESCRIPTION\\n'\n",
            "                '\\n'\n",
            "                'The Yelp reviews polarity dataset is constructed by '\n",
            "                'considering stars 1 and 2\\n'\n",
            "                'negative, and 3 and 4 positive. For each polarity 280,000 '\n",
            "                'training samples and\\n'\n",
            "                '19,000 testing samples are take randomly. In total there are '\n",
            "                '560,000 trainig\\n'\n",
            "                'samples and 38,000 testing samples. Negative polarity is '\n",
            "                'class 1,\\n'\n",
            "                'and positive class 2.\\n'\n",
            "                '\\n'\n",
            "                'The files train.csv and test.csv contain all the training '\n",
            "                'samples as\\n'\n",
            "                'comma-sparated values. There are 2 columns in them, '\n",
            "                'corresponding to class\\n'\n",
            "                'index (1 and 2) and review text. The review texts are escaped '\n",
            "                'using double\\n'\n",
            "                'quotes (\"), and any internal double quote is escaped by 2 '\n",
            "                'double quotes (\"\").\\n'\n",
            "                'New lines are escaped by a backslash followed with an \"n\" '\n",
            "                'character,\\n'\n",
            "                'that is \"\\\\n\".',\n",
            " 'etag': '\"20d1bdc9fd68912cb0ce9f1cf1f42adc\"',\n",
            " 'id': 'yelp_polarity',\n",
            " 'key': 'datasets/datasets/yelp_polarity/yelp_polarity.py',\n",
            " 'lastModified': '2020-09-15T08:26:33.000Z',\n",
            " 'numModels': 1,\n",
            " 'siblings': [datasets.S3Object('dataset_infos.json'),\n",
            "              datasets.S3Object('dummy/plain_text/1.0.0/dummy_data.zip'),\n",
            "              datasets.S3Object('yelp_polarity.py')],\n",
            " 'size': 5947}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60k916LHhQ94"
      },
      "source": [
        "Let us now download and load yelp dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m2ZmafohQ94",
        "outputId": "27e21833-8d10-4336-e6a6-2e037de6c67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('yelp_polarity', split='test[:1%]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lr2uRoQhQ98"
      },
      "source": [
        "This call to `datasets.load_dataset()` does the following steps under the hood:\n",
        "\n",
        "1. Download and import in the library the **Yelp polarity python processing script** from HuggingFace AWS bucket if it's not already stored in the library. You can find the Yelp processing script [here](https://github.com/huggingface/datasets/blob/master/datasets/yelp_polarity/yelp_polarity.py) for instance.\n",
        "\n",
        "   Processing scripts are small python scripts which define the info (citation, description) and format of the dataset and contain the URL to the original Yelp JSON files and the code to load examples from the original Yelp JSON files.\n",
        "\n",
        "\n",
        "2. Run the Yelp python processing script which will:\n",
        "    - **Download the Yelp dataset** from the original URL (see the script) if it's not already downloaded and cached.\n",
        "    - **Process and cache** all Yelp reviews in a structured Arrow table for each standard splits stored on the drive.\n",
        "\n",
        "      Arrow table are arbitrarily long tables, typed with types that can be mapped to numpy/pandas/python standard types and can store nested objects. They can be directly access from drive, loaded in RAM or even streamed over the web.\n",
        "    \n",
        "\n",
        "3. Return a **dataset built from the splits** asked by the user (default: all), in the above example we create a dataset with the first 1% of the test split.\n",
        "\n",
        "The returned `Dataset` object is a memory mapped dataset that behave similarly to a normal map-style dataset. It is backed by an Apache Arrow table which allows many interesting features.\n",
        "\n",
        "Let us get information on the dataset (description, citation, size, splits, format, ...):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzIwomQhQ99",
        "outputId": "82545d97-e45d-4198-ed22-be8c5832881d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# are provided in `dataset.info` (a simple python dataclass) and also as direct attributes in the dataset object\n",
        "pprint(dataset.info.__dict__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'builder_name': 'yelp_polarity',\n",
            " 'citation': '@article{zhangCharacterlevelConvolutionalNetworks2015,\\n'\n",
            "             '  archivePrefix = {arXiv},\\n'\n",
            "             '  eprinttype = {arxiv},\\n'\n",
            "             '  eprint = {1509.01626},\\n'\n",
            "             '  primaryClass = {cs},\\n'\n",
            "             '  title = {Character-Level {{Convolutional Networks}} for {{Text '\n",
            "             'Classification}}},\\n'\n",
            "             '  abstract = {This article offers an empirical exploration on '\n",
            "             'the use of character-level convolutional networks (ConvNets) for '\n",
            "             'text classification. We constructed several large-scale datasets '\n",
            "             'to show that character-level convolutional networks could '\n",
            "             'achieve state-of-the-art or competitive results. Comparisons are '\n",
            "             'offered against traditional models such as bag of words, n-grams '\n",
            "             'and their TFIDF variants, and deep learning models such as '\n",
            "             'word-based ConvNets and recurrent neural networks.},\\n'\n",
            "             '  journal = {arXiv:1509.01626 [cs]},\\n'\n",
            "             '  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n'\n",
            "             '  month = sep,\\n'\n",
            "             '  year = {2015},\\n'\n",
            "             '}\\n'\n",
            "             '\\n',\n",
            " 'config_name': 'plain_text',\n",
            " 'dataset_size': 441521174,\n",
            " 'description': 'Large Yelp Review Dataset.\\n'\n",
            "                'This is a dataset for binary sentiment classification. We '\n",
            "                'provide a set of 560,000 highly polar yelp reviews for '\n",
            "                'training, and 38,000 for testing. \\n'\n",
            "                'ORIGIN\\n'\n",
            "                'The Yelp reviews dataset consists of reviews from Yelp. It is '\n",
            "                'extracted\\n'\n",
            "                'from the Yelp Dataset Challenge 2015 data. For more '\n",
            "                'information, please\\n'\n",
            "                'refer to http://www.yelp.com/dataset_challenge\\n'\n",
            "                '\\n'\n",
            "                'The Yelp reviews polarity dataset is constructed by\\n'\n",
            "                'Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\n'\n",
            "                'It is first used as a text classification benchmark in the '\n",
            "                'following paper:\\n'\n",
            "                'Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level '\n",
            "                'Convolutional Networks\\n'\n",
            "                'for Text Classification. Advances in Neural Information '\n",
            "                'Processing Systems 28\\n'\n",
            "                '(NIPS 2015).\\n'\n",
            "                '\\n'\n",
            "                '\\n'\n",
            "                'DESCRIPTION\\n'\n",
            "                '\\n'\n",
            "                'The Yelp reviews polarity dataset is constructed by '\n",
            "                'considering stars 1 and 2\\n'\n",
            "                'negative, and 3 and 4 positive. For each polarity 280,000 '\n",
            "                'training samples and\\n'\n",
            "                '19,000 testing samples are take randomly. In total there are '\n",
            "                '560,000 trainig\\n'\n",
            "                'samples and 38,000 testing samples. Negative polarity is '\n",
            "                'class 1,\\n'\n",
            "                'and positive class 2.\\n'\n",
            "                '\\n'\n",
            "                'The files train.csv and test.csv contain all the training '\n",
            "                'samples as\\n'\n",
            "                'comma-sparated values. There are 2 columns in them, '\n",
            "                'corresponding to class\\n'\n",
            "                'index (1 and 2) and review text. The review texts are escaped '\n",
            "                'using double\\n'\n",
            "                'quotes (\"), and any internal double quote is escaped by 2 '\n",
            "                'double quotes (\"\").\\n'\n",
            "                'New lines are escaped by a backslash followed with an \"n\" '\n",
            "                'character,\\n'\n",
            "                'that is \"\\n'\n",
            "                '\".\\n',\n",
            " 'download_checksums': {'https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz': {'checksum': '528f22e286cad085948acbc3bea7e58188416546b0e364d0ae4ca0ce666abe35',\n",
            "                                                                                              'num_bytes': 166373201}},\n",
            " 'download_size': 166373201,\n",
            " 'features': {'label': ClassLabel(num_classes=2, names=['1', '2'], names_file=None, id=None),\n",
            "              'text': Value(dtype='string', id=None)},\n",
            " 'homepage': 'https://course.fast.ai/datasets',\n",
            " 'license': '',\n",
            " 'post_processed': None,\n",
            " 'post_processing_size': None,\n",
            " 'size_in_bytes': 607894375,\n",
            " 'splits': {'test': SplitInfo(name='test', num_bytes=27962113, num_examples=38000, dataset_name='yelp_polarity'),\n",
            "            'train': SplitInfo(name='train', num_bytes=413559061, num_examples=560000, dataset_name='yelp_polarity')},\n",
            " 'supervised_keys': None,\n",
            " 'version': 1.0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3-98UmLhQ-A"
      },
      "source": [
        "### Inspect and modify yelp dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK-5zHYChQ-B"
      },
      "source": [
        "Let's pretty print the dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPIRGBE-hQ-B",
        "outputId": "08383688-a22b-4f75-cebd-e2822e37df92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['1', '2'], names_file=None, id=None)}, num_rows: 380)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LWMxGJChQ-E"
      },
      "source": [
        "We can query it's length like we would do normally with a python mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5eE9i7FhQ-E",
        "outputId": "6ccb2f1e-3a00-4c54-9665-f9c19a9b8497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"ðŸ‘‰Dataset len(dataset): {len(dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ‘‰Dataset len(dataset): 380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr4u0fFZhQ-H"
      },
      "source": [
        "We can get items or slices like we would do normally with a python mapping. Let us get the first example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Zmprq_hQ-H",
        "outputId": "0769e16a-b1e3-490b-9d47-dac4932054fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\\nðŸ‘‰First item 'dataset[0]':\")\n",
        "pprint(dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ‘‰First item 'dataset[0]':\n",
            "{'label': 1,\n",
            " 'text': 'Contrary to other reviews, I have zero complaints about the service '\n",
            "         'or the prices. I have been getting tire service here for the past 5 '\n",
            "         'years now, and compared to my experience with places like Pep Boys, '\n",
            "         \"these guys are experienced and know what they're doing. \\\\nAlso, \"\n",
            "         'this is one place that I do not feel like I am being taken advantage '\n",
            "         'of, just because of my gender. Other auto mechanics have been '\n",
            "         'notorious for capitalizing on my ignorance of cars, and have sucked '\n",
            "         'my bank account dry. But here, my service and road coverage has all '\n",
            "         'been well explained - and let up to me to decide. \\\\nAnd they just '\n",
            "         'renovated the waiting room. It looks a lot better than it did in '\n",
            "         'previous years.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3eOIrdhQ-K"
      },
      "source": [
        "Let us slice several examples (8th, 9th, 10th):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w0j2NY2hQ-L",
        "outputId": "54f5fca8-a479-4f01-ee38-93fdde6e5d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\\nðŸ‘‰Slice of the three items 'dataset[7:10]':\")\n",
        "pprint(dataset[7:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ‘‰Slice of the three items 'dataset[7:10]':\n",
            "OrderedDict([('label', [0, 0, 1]),\n",
            "             ('text',\n",
            "              ['Ok! Let me tell you about my bad experience first. I went to '\n",
            "               'D&B last night for a post wedding party - which, side note, is '\n",
            "               \"a great idea!\\\\n\\\\nIt was around midnight and the bar wasn't \"\n",
            "               'really populated. There were three bartenders and only one was '\n",
            "               'actually making rounds to see if anyone needed anything. The '\n",
            "               'two other bartenders were chatting on the far side of the bar '\n",
            "               'that no one was sitting at. Kind of counter productive if you '\n",
            "               'ask me. \\\\n\\\\nI stood there for about 5 minutes, which for a '\n",
            "               'busy bar is fine but when I am the only one with my card out '\n",
            "               'then, it just seems a little ridiculous. I made eye contact '\n",
            "               'with the one girl twice and gave her a smile and she literally '\n",
            "               'turned away. I finally had to walk to them to get their '\n",
            "               'attention.  I was standing right in front of them smiling and '\n",
            "               'they didn\\'t ask if i need anything. I finally said, \\\\\"\"Are '\n",
            "               'you working?\\\\\"\" and they gave each other a weird look. I felt '\n",
            "               'like i was the crazy one. I asked for a beer/got the '\n",
            "               'beer.\\\\n\\\\nIn between that time, the other bartender brought '\n",
            "               'food over and set it down. She took a fry from the plate '\n",
            "               '(right in front of me) and then served it to someone on the '\n",
            "               'other side of the bar. What the hell! I felt like i was in '\n",
            "               'some grimy bar in out in the sticks - not an established D&B. '\n",
            "               '\\\\n\\\\nI was just really turned off from that experience. '\n",
            "               '\\\\n\\\\nThe good is that D&B provides a different type of '\n",
            "               'entertainment when you want to mix things up. I remember going '\n",
            "               'here with my grandparents when I was a kid and it was the best '\n",
            "               'treat ever! We would eat at the restaurant and then spend '\n",
            "               'hours playing games. This place holds some really good '\n",
            "               \"memories for me. \\\\n\\\\nIt's a shame that my experience last \"\n",
            "               'night has spoiled the high standards I held for it.',\n",
            "               'I used to love D&B when it first opened in the Waterfront, but '\n",
            "               'it has gone down hill over the years. The games are not as fun '\n",
            "               'and do not give you as many tickets and the prizes have gotten '\n",
            "               'cheaper in quality. It takes a whole heck of a lot of tickets '\n",
            "               'for you to even get a pencil! The atmosphere is okay but it '\n",
            "               'used to be so much better with the funnest games and diverse '\n",
            "               'groups of people! Now, it is run down and many of the games '\n",
            "               'are app related games (Fruit Ninja) and 3D Experience rides. '\n",
            "               'With such \\\\\"\"games\\\\\"\", you can\\'t even earn tickets and they '\n",
            "               'take a lot of tokens! Last time I went, back in the winter, '\n",
            "               'many of the games were broken, which made for a negative '\n",
            "               'player experience. I would go to D&B to play some games again '\n",
            "               'in the future, but it is no longer one of my favorite places '\n",
            "               'to go due to the decline of fun games where you can earn '\n",
            "               'tickets.',\n",
            "               'Like any Barnes & Noble, it has a nice comfy cafe, and a large '\n",
            "               'selection of books.  The staff is very friendly and helpful.  '\n",
            "               'They stock a decent selection, and the prices are pretty '\n",
            "               \"reasonable.  Obviously it's hard for them to compete with \"\n",
            "               'Amazon.  However since all the small shop bookstores are gone, '\n",
            "               \"it's nice to walk into one every once in a while.\"])])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMA6cSwdhQ-O"
      },
      "source": [
        "Let us get a full column of the dataset for first 5 examples by indexing with its name as a string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naWXQv_dhQ-P",
        "outputId": "0d6825d3-8f6e-449c-9e90-b7ef0b4ed305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(dataset['text'][0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Contrary to other reviews, I have zero complaints about the service or the '\n",
            " 'prices. I have been getting tire service here for the past 5 years now, and '\n",
            " 'compared to my experience with places like Pep Boys, these guys are '\n",
            " \"experienced and know what they're doing. \\\\nAlso, this is one place that I \"\n",
            " 'do not feel like I am being taken advantage of, just because of my gender. '\n",
            " 'Other auto mechanics have been notorious for capitalizing on my ignorance of '\n",
            " 'cars, and have sucked my bank account dry. But here, my service and road '\n",
            " 'coverage has all been well explained - and let up to me to decide. \\\\nAnd '\n",
            " 'they just renovated the waiting room. It looks a lot better than it did in '\n",
            " 'previous years.',\n",
            " 'Last summer I had an appointment to get new tires and had to wait a super '\n",
            " 'long time. I also went in this week for them to fix a minor problem with a '\n",
            " 'tire they put on. They \\\\\"\"fixed\\\\\"\" it for free, and the very next morning '\n",
            " 'I had the same issue. I called to complain, and the \\\\\"\"manager\\\\\"\" didn\\'t '\n",
            " 'even apologize!!! So frustrated. Never going back.  They seem overpriced, '\n",
            " 'too.',\n",
            " 'Friendly staff, same starbucks fair you get anywhere else.  Sometimes the '\n",
            " 'lines can get long.',\n",
            " 'The food is good. Unfortunately the service is very hit or miss. The main '\n",
            " 'issue seems to be with the kitchen, the waiters and waitresses are often '\n",
            " \"very apologetic for the long waits and it's pretty obvious that some of them \"\n",
            " 'avoid the tables after taking the initial order to avoid hearing complaints.',\n",
            " \"Even when we didn't have a car Filene's Basement was worth the bus trip to \"\n",
            " 'the Waterfront. I always find something (usually I find 3-4 things and spend '\n",
            " 'about $60) and better still, I am always still wearing the clothes and shoes '\n",
            " '3 months later. \\\\n\\\\nI kind of suspect this is the best shopping in '\n",
            " \"Pittsburgh; it's much better than the usual department stores, better than \"\n",
            " \"Marshall's and TJ Maxx and better than the Saks downtown, even when it has a \"\n",
            " \"sale. Selection, bargains AND quality.\\\\n\\\\nI like this Filene's better than \"\n",
            " 'Gabriel Brothers, which are harder to get to. Gabriel Brothers are a real '\n",
            " \"discount shopper's challenge and I'm afraid I didn't live in Pittsburgh long \"\n",
            " \"enough to develop the necessary skills . . . Filene's was still up and \"\n",
            " 'running in June 2007 when I left town.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gjR_ptjhQ-S"
      },
      "source": [
        "The `__getitem__` method will return different format depending on the type of query:\n",
        "\n",
        "- Items like `dataset[0]` are returned as dict of elements.\n",
        "- Slices like `dataset[7:10]` are returned as dict of lists of elements.\n",
        "- Columns like `dataset['text']` are returned as a list of elements.\n",
        "\n",
        "In particular, we can easily iterate along columns in slices, and also naturally permute consecutive indexings with identical results as showed here by permuting column indexing with elements and slices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HigCsn_YhQ-S",
        "outputId": "4b5323e9-af21-49b5-fc1b-f527f814f703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(dataset[0]['text'])\n",
        "print(dataset['text'][0]) # returns same result as previous command\n",
        "print(dataset[0]['text'] == dataset['text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
            "Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgFXqGThQ-V"
      },
      "source": [
        "Similarly we can apply permuation for slice of multiple indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-qSnjjhQ-W",
        "outputId": "c936caf4-5875-44f4-e234-3c09fb631568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(dataset[0:5]['text'] == dataset['text'][0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Jt1yrKhQ-Y"
      },
      "source": [
        "#### Dataset are internally typed and structured\n",
        "\n",
        "The dataset is backed by one (or several) Apache Arrow tables which are typed and allows for fast retrieval and access as well as arbitrary-size memory mapping.\n",
        "\n",
        "This means respectively that the format for the dataset is clearly defined and that you can load datasets of arbitrary size without worrying about RAM memory limitation (basically the dataset take no space in RAM, it's directly read from drive when needed with fast IO access).\n",
        "\n",
        "We can inspect the dataset column names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W631IKhBhQ-Y",
        "outputId": "328c8f26-a73f-4f69-b76d-84428be5d329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Column names:\")\n",
        "pprint(dataset.column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column names:\n",
            "['label', 'text']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQcN5ftYhQ-b"
      },
      "source": [
        "We can inspect the dataset column types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VswUf-qhQ-e",
        "outputId": "319b89bd-8ad7-48e9-8c7d-4f5ec46f3de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Features:\")\n",
        "pprint(dataset.features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:\n",
            "{'label': ClassLabel(num_classes=2, names=['1', '2'], names_file=None, id=None),\n",
            " 'text': Value(dtype='string', id=None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UHjKR0HhQ-i"
      },
      "source": [
        "#### Modifying the dataset with `dataset.map`\n",
        "\n",
        "Now that we know how to inspect our dataset we also want to update it. For that there is a powerful method `.map()` that we can use to apply a function to each examples, independently or in batch.\n",
        "\n",
        "`.map()` takes a callable accepting a dict as argument (same dict as the one returned by `dataset[i]`) and iterate over the dataset by calling the function on each example.\n",
        "\n",
        "Let us use map function to print length of all texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHozCs4hQ-j",
        "outputId": "3412befa-f4ee-4a22-e0da-00ec2d0310a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "d1a67fa6718b4a12890b040ab9c841fd",
            "6835da877c93409fb85cd6107f1405c7",
            "77f14e964fb54bb6a0d61ab867809838",
            "61e4b60466374ef8a9d3243272ed4e7d",
            "8d882d2bf97949cfa3dc67cc3d840470",
            "ea8e5ba248224bc0aa9399d86ec6fd65",
            "388408a8f5fa4151b9d0676e5a17625e",
            "f9bf093a92404906b6268a7331fdc182"
          ]
        }
      },
      "source": [
        "dataset.map(lambda example: print(len(example['text']), end=','))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "681,"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1a67fa6718b4a12890b040ab9c841fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=380.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "681,374,93,300,783,1069,198,1720,858,352,1063,922,1124,632,217,199,2230,317,1157,414,862,505,153,1165,170,1772,575,1166,114,548,627,286,1133,725,261,828,657,2853,88,727,1008,383,176,617,180,439,1637,1963,1625,749,1693,1773,212,4125,118,266,858,403,564,845,257,1321,1176,1395,566,409,177,188,241,266,63,373,225,1040,283,813,189,244,499,184,124,1655,340,845,1348,167,2874,407,560,271,2837,190,476,55,1192,30,1727,1216,486,267,363,470,710,145,459,482,165,847,900,1237,1186,452,903,575,1993,445,414,1131,1686,1141,396,881,433,255,282,681,583,974,1361,3354,563,1333,885,845,407,718,509,207,334,458,305,1504,641,317,498,465,410,881,768,228,441,426,119,1863,878,238,2260,642,2246,158,941,922,175,823,699,427,239,122,146,1176,238,1102,423,125,1742,106,356,614,269,1555,348,1223,636,790,689,502,143,291,201,275,144,1005,649,1242,207,559,397,442,1049,1263,184,720,355,504,788,1316,2220,275,1051,921,373,431,753,784,1733,133,45,122,1084,43,492,414,203,187,710,611,1482,112,401,980,764,1578,206,650,992,708,642,564,2618,1404,838,115,611,631,2105,694,383,1603,1031,300,1237,381,1114,1412,439,1413,464,684,1689,273,2950,230,34,65,251,1153,394,262,676,484,49,1461,149,1407,420,1267,169,1136,155,996,348,706,898,419,518,148,279,865,769,2048,1043,2140,414,890,659,1136,276,491,1505,785,965,282,276,400,1100,606,846,518,772,231,164,252,1079,575,1406,1513,1195,647,313,898,99,1354,994,200,2078,271,565,1179,796,317,1006,582,582,345,1494,1575,1953,480,280,987,200,972,371,219,135,1360,1269,517,216,208,495,1286,608,1241,466,199,216,142,619,567,1025,375,1005,2266,531,117,1006,1355,1074,974,302,371,482,1081,761,731,313,291,377,562,\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['1', '2'], names_file=None, id=None)}, num_rows: 380)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVhqzUZhQ-l"
      },
      "source": [
        "This is basically the same as doing\n",
        "\n",
        "```python\n",
        "for example in dataset:\n",
        "    function(example)\n",
        "```\n",
        "\n",
        "The main interest of `.map()` is to update and modify the content of the table and leverage smart caching and fast backend.\n",
        "\n",
        "To use `.map()` to update elements in the table you need to provide a function with the following signature: `function(example: dict) -> dict`.\n",
        "\n",
        "Let us try to add a new field `cute_text` that contains prefix `My cute review:` to all reviews in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyZMqiVihQ-m",
        "outputId": "8cfba976-e7b2-478a-e94c-cbf2f08a3f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def add_prefix_to_text(example):\n",
        "    example['cute_text'] = 'My cute review: ' + example['text']\n",
        "    return example\n",
        "\n",
        "prefixed_dataset = dataset.map(add_prefix_to_text)\n",
        "print(prefixed_dataset.column_names) # print column names\n",
        "pprint(prefixed_dataset.unique('cute_text')[0:3])  # `.unique()` is a super fast way to print the unique elemnts in a column (see the doc for all the methods)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-b88eb39dd560101c.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['cute_text', 'label', 'text']\n",
            "['My cute review: Contrary to other reviews, I have zero complaints about the '\n",
            " 'service or the prices. I have been getting tire service here for the past 5 '\n",
            " 'years now, and compared to my experience with places like Pep Boys, these '\n",
            " \"guys are experienced and know what they're doing. \\\\nAlso, this is one place \"\n",
            " 'that I do not feel like I am being taken advantage of, just because of my '\n",
            " 'gender. Other auto mechanics have been notorious for capitalizing on my '\n",
            " 'ignorance of cars, and have sucked my bank account dry. But here, my service '\n",
            " 'and road coverage has all been well explained - and let up to me to decide. '\n",
            " '\\\\nAnd they just renovated the waiting room. It looks a lot better than it '\n",
            " 'did in previous years.',\n",
            " 'My cute review: Last summer I had an appointment to get new tires and had to '\n",
            " 'wait a super long time. I also went in this week for them to fix a minor '\n",
            " 'problem with a tire they put on. They \\\\\"\"fixed\\\\\"\" it for free, and the '\n",
            " 'very next morning I had the same issue. I called to complain, and the '\n",
            " '\\\\\"\"manager\\\\\"\" didn\\'t even apologize!!! So frustrated. Never going back.  '\n",
            " 'They seem overpriced, too.',\n",
            " 'My cute review: Friendly staff, same starbucks fair you get anywhere else.  '\n",
            " 'Sometimes the lines can get long.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-RLu3wDhQ-q"
      },
      "source": [
        "The function you provide to `.map()` should accept an input with the format of an item of the dataset: `function(dataset[0])` and return a python dict.\n",
        "\n",
        "Let us remove the column `cute_text` by running map with the `remove_columns=List[str]` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUrtkWwqhQ-q",
        "outputId": "92cf6a6d-3d4e-4d76-9765-04dbef7f6587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "less_columns_dataset = prefixed_dataset.map(remove_columns=['cute_text'])\n",
        "print(less_columns_dataset.column_names) # print column names\n",
        "pprint(less_columns_dataset.unique('text')[0:3]) # print three texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-56c4ecfed98f304d.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['label', 'text']\n",
            "['Contrary to other reviews, I have zero complaints about the service or the '\n",
            " 'prices. I have been getting tire service here for the past 5 years now, and '\n",
            " 'compared to my experience with places like Pep Boys, these guys are '\n",
            " \"experienced and know what they're doing. \\\\nAlso, this is one place that I \"\n",
            " 'do not feel like I am being taken advantage of, just because of my gender. '\n",
            " 'Other auto mechanics have been notorious for capitalizing on my ignorance of '\n",
            " 'cars, and have sucked my bank account dry. But here, my service and road '\n",
            " 'coverage has all been well explained - and let up to me to decide. \\\\nAnd '\n",
            " 'they just renovated the waiting room. It looks a lot better than it did in '\n",
            " 'previous years.',\n",
            " 'Last summer I had an appointment to get new tires and had to wait a super '\n",
            " 'long time. I also went in this week for them to fix a minor problem with a '\n",
            " 'tire they put on. They \\\\\"\"fixed\\\\\"\" it for free, and the very next morning '\n",
            " 'I had the same issue. I called to complain, and the \\\\\"\"manager\\\\\"\" didn\\'t '\n",
            " 'even apologize!!! So frustrated. Never going back.  They seem overpriced, '\n",
            " 'too.',\n",
            " 'Friendly staff, same starbucks fair you get anywhere else.  Sometimes the '\n",
            " 'lines can get long.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4svxkfx7hQ-u"
      },
      "source": [
        "#### Train a BERT model using yelp dataset\n",
        "\n",
        "Let us start by tokenizing 1% of train dataset. For that, we need to load train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xM4btOhQ-v",
        "outputId": "7b82b446-d348-4b04-8bbc-40244e07740c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset = load_dataset('yelp_polarity', split='train[:1%]')\n",
        "pprint(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset yelp_polarity (/root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['1', '2'], names_file=None, id=None)}, num_rows: 5600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O9fhygVhQ-y"
      },
      "source": [
        "Now let us try to tokenize all the reviews. We will use `Tokenizer` from transformers library.\n",
        "\n",
        "Input to Tokenizer: The tokenizers of the ðŸ¤—transformers library can accept lists of texts as inputs and tokenize them efficiently in batch (for the fast tokenizers in particular).\n",
        "\n",
        "Output to Tokenizer: This tokenizer will output a dictionary-like object with three fields: input_ids, token_type_ids, attention_mask corresponding to modelâ€™s required inputs. Each field contain a list (batch) of samples.\n",
        "\n",
        "Let's load the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNq3lQg0hQ-z",
        "outputId": "b79a9af0-024a-4747-8ff7-b67861732f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoq_AJOAhQ-1"
      },
      "source": [
        "Let's tokenize all the reviews now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Iw_YPihQ-2",
        "outputId": "38698a2a-86a9-4e84-b5a4-9d88aa89a20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded_train_dataset = train_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-e9bb4a6ed5bff8ae.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0US48_W4hQ-5"
      },
      "source": [
        "Let's look at the column names in the `encoded_train_dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMZ8ucC1hQ-5",
        "outputId": "aeb2b67c-53df-4209-a539-46eb4076a30a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(encoded_train_dataset.column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHQ6kEY2hQ-8"
      },
      "source": [
        "Here goes some details of each column name:\n",
        "\n",
        "`attention_mask`: List of indices specifying which tokens should be attended to by the model\n",
        "\n",
        "`input_ids`:  List of token ids to be fed to a model.\n",
        "\n",
        "`token_type_ids`: List of token type ids to be fed to a model \n",
        "\n",
        "For more details of above, check https://huggingface.co/transformers/main_classes/tokenizer.html\n",
        "\n",
        "`text`: Raw review\n",
        "\n",
        "`label`: Raw label\n",
        "\n",
        "Let's print one example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJkOHM0IhQ-9",
        "outputId": "4ee5bdc8-6f71-46da-839d-30593d4d20d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(encoded_train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
            " 'input_ids': tensor([  101,  7595,   117,  1103,  9074,  1104,  1217,  1987,   119, 20029,\n",
            "          112,   188,  5351,  1110,   170,  9488,  1104,  1103,  2541,   146,\n",
            "          112,  1396,  1125,  1114,  1177,  1242,  1168,  8114,  1107, 17520,\n",
            "          118,   118,  1363,  3995,   117,  6434,  2546,   119,  1135,  3093,\n",
            "         1115,  1117,  2546,  2566,  1309,  6615,  1103,  2179,   119,  1135,\n",
            "         1932,  2274,   123,  2005,  1104,  4892,  3516,  1106,  1243,  1126,\n",
            "         2590,   119,  2627,  1144,  1159,  1111,  1115,  1137,  3349,  1106,\n",
            "         2239,  1114,  1122,   136,   146,  1138,  1576,  1154,  1142,  2463,\n",
            "         1114,  1242,  1168,  8114,  1105,   146,  1198,  1274,   112,   189,\n",
            "         1243,  1122,   119,  1192,  1138,  1701,  3239,   117,  1128,  1138,\n",
            "         4420,  1114,  2657,  2993,   117,  1725,  2762,   112,   189,  2256,\n",
            "        10937,  1103,  2179,   136,  1135,   112,   188,  1107,  8178,  1643,\n",
            "         1874, 10436, 24355,  1105,  1136,  1250,  1103,   170,  9705,  1611,\n",
            "        11583,   119,  1135,   112,   188,  1114,  9005,  1115,   146,  1631,\n",
            "         1115,   146,  1138,  1106,  1660,  1987,   119, 20029,   123,  2940,\n",
            "          119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]),\n",
            " 'label': tensor(0)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBHcOhXOhQ_C"
      },
      "source": [
        "#### formatting outputs for PyTorch, Tensorflow, Numpy, Pandas\n",
        "\n",
        "Now that we have tokenized our inputs, we probably want to use this dataset in a `torch.Dataloader`\n",
        "\n",
        "To be able to do this we need to tweak two things:\n",
        "\n",
        "- format the indexing (`__getitem__`) to return numpy/pytorch/tensorflow tensors, instead of python objects, and probably\n",
        "- format the indexing (`__getitem__`) to return only the subset of the columns that we need for our model inputs.\n",
        "\n",
        "  We don't want the columns `id` or `title` as inputs to train our model, but we could still want to keep them in the dataset, for instance for the evaluation of the model.\n",
        "    \n",
        "This is handled by the `.set_format(type: Union[None, str], columns: Union[None, str, List[str]])` where:\n",
        "\n",
        "- `type` define the return type for our dataset `__getitem__` method and is one of `[None, 'numpy', 'pandas', 'torch', 'tensorflow']` (`None` means return python objects), and\n",
        "- `columns` define the columns returned by `__getitem__` and takes the name of a column in the dataset or a list of columns to return (`None` means return all columns).\n",
        "\n",
        "Let us list the columns required for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WN0gzvghQ_C"
      },
      "source": [
        "columns_to_return = ['input_ids', 'attention_mask', 'label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCnaQByUhQ_F"
      },
      "source": [
        "Let us change the format of the dataset to torch (suitable for `torch.Dataloader`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sfwgs3BhQ_F"
      },
      "source": [
        "encoded_train_dataset.set_format(type='torch', columns=columns_to_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJk6cZgrhQ_J"
      },
      "source": [
        "Our dataset indexing output is now ready for being used in a pytorch dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DkKvRSshQ_J",
        "outputId": "886babc1-d6e1-4ada-d4aa-cda55d5041ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(encoded_train_dataset[1], compact=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
            " 'input_ids': tensor([  101, 18511,  1280,  1106,  1987,   119, 20029,  1111,  1166,  1275,\n",
            "         1201,   119,   146,  1341,   146,  1108,  1141,  1104,  1117,  2198,\n",
            "         4420,  1165,  1119,  1408,  1120,   150,  3048, 14666,   119,  1124,\n",
            "          112,   188,  1151,  1632,  1166,  1103,  1201,  1105,  1110,  1541,\n",
            "         1155,  1164,  1103,  1992,  3439,   119,  1135,  1110,  1272,  1104,\n",
            "         1140,   117,  1136,  1139,  1208,  1393,   176,  5730,  1987,   119,\n",
            "         2392,  5792,   117,  1115,   146,  1276,  1149,   146,  1138, 20497,\n",
            "        12725,  7540,   119,  1124, 16001,  1155,  6665,  1114,  1128,  1105,\n",
            "         1110,  1304,  5351,  1105,  4287,   119,  1124,  2144,   112,   189,\n",
            "         3942,  1105,  4390,  1155,  1103,  1268,  3243,   119,  6424, 17213,\n",
            "         1105,  3349,  1106,  1129,  2023,  1107,  1103,  7812,  1113,  1451,\n",
            "         7631,  1104,  1240,  2657,  2332,  1105,  1240,  1297,   119,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]),\n",
            " 'label': tensor(1)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.tensor(x, **format_kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4967_J6ChQ_M"
      },
      "source": [
        "Let's instantiate a data loader that consumes our dataset indexing output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8yzjfrshQ_M"
      },
      "source": [
        "import torch\n",
        "dataloader = torch.utils.data.DataLoader(encoded_train_dataset, batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdQVqL3zhQ_O"
      },
      "source": [
        "Let's import Bert base model, Adam optimizer and loss function from transformers library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6JIsJRhQ_P",
        "outputId": "2833aab3-d575-46d3-bf22-a6a33846e863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2, output_attentions = False, output_hidden_states = False,)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBt0KpachQ_S"
      },
      "source": [
        "Let's train BertModel base model on our tokenized dataset for 1 step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxdItnzphQ_S",
        "outputId": "27e584ea-2c29-45f7-d8d3-71080124c653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.train() # toggle training mode\n",
        "for i, batch in enumerate(dataloader):\n",
        "    loss, logits = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label']) # pass the input batch to model\n",
        "    loss.backward() # backprop the grads (store in grad buffer)\n",
        "    optimizer.step() # use grads from grad buffer to update the model\n",
        "    model.zero_grad() # zero the grad buffer\n",
        "    print(f'Step {i} - loss: {loss.item():.3}') # print the step loss\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 - loss: 0.715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvQSnaVahQ_V"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "`datasets` also provides easy access and sharing of metrics.\n",
        "\n",
        "This aspect of the library is still experimental and the API may still evolve more than the datasets API.\n",
        "\n",
        "Like datasets, metrics are added as small scripts wrapping common metrics in a common API.\n",
        "\n",
        "There are several reason you may want to use metrics with `datasets` and in particular:\n",
        "\n",
        "- metrics for specific datasets like GLUE or SQuAD are provided out-of-the-box in a simple, convenient and consistant way integrated with the dataset,\n",
        "- metrics in `datasets` leverage the powerful backend to provide smart features out-of-the-box like support for distributed evaluation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzg3G4DHhQ_W"
      },
      "source": [
        "Let's list available metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE74TvEohQ_X",
        "outputId": "b124e3a1-29af-471c-ca44-0743eb5007aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "metrics = list_metrics()\n",
        "print(f\"Currently {len(metrics)} metrics are available on the hub:\")\n",
        "pprint(metrics, compact=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently 13 metrics are available on the hub:\n",
            "['bertscore', 'bleu', 'bleurt', 'coval', 'gleu', 'glue', 'meteor', 'rouge',\n",
            " 'sacrebleu', 'seqeval', 'squad', 'squad_v2', 'xnli']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULPzYHyrhQ_Z"
      },
      "source": [
        "Let's look at an example metric: `seqeval`. `seqeval` is a Python framework for sequence labeling evaluation that can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on. \n",
        "\n",
        "For more details about `seqeval` metric, look at: https://huggingface.co/metrics/seqeval\n",
        "\n",
        "Let's install the dependency for the metric and load that metric now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6DXZEVZhQ_a",
        "outputId": "bd119372-8b04-4e2f-a147-81f893c5f956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install seqeval \n",
        "ner_metric = load_metric('seqeval')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL3cUz3fhQ_c"
      },
      "source": [
        "Let's generate sample references and predictions for NER task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihjdv2YzhQ_d"
      },
      "source": [
        "references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "predictions =  [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tehj3TYshQ_f"
      },
      "source": [
        "Let's use `seqeval` metric to score the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5DeAsAhQ_f",
        "outputId": "f0503f94-53a4-488c-f681-da83f9bd6532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ner_metric.compute(predictions=predictions, references=references)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MISC': {'f1': 0, 'number': 1, 'precision': 0.0, 'recall': 0.0},\n",
              " 'PER': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},\n",
              " 'overall_accuracy': 0.8,\n",
              " 'overall_f1': 0.5,\n",
              " 'overall_precision': 0.5,\n",
              " 'overall_recall': 0.5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2WVMPX3hQ_j"
      },
      "source": [
        "## Loading local content\n",
        "\n",
        "Itâ€™s also possible to create a dataset from local files or in-memory data.\n",
        "\n",
        "There are multiple file types which is currently supported which we can use:\n",
        "- CSV files \n",
        "- JSON files \n",
        "- text files (read as a line-by-line dataset)\n",
        "- pandas pickled dataframe\n",
        "\n",
        "\n",
        "To begin, we need to import Dataset class from the library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI9JsSyYnWOf"
      },
      "source": [
        "from datasets import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWK0Oi6SnWZ5"
      },
      "source": [
        "### Loading from a python dictionary\n",
        "\n",
        "Let's create a python dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll561as8nBoq"
      },
      "source": [
        "my_dict = {'id': [0, 1, 2, 3], 'name': ['mary', 'bob', 'eve', 'rob'], 'age': [24, 53, 19, 25]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XW0ZD1GnE7P"
      },
      "source": [
        "Let's instantiate a dataset object from the above dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q5STa4knDlT",
        "outputId": "6d699e7f-4242-42af-b6a3-3de3af6e2a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = Dataset.from_dict(my_dict)\n",
        "pprint(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(features: {'id': Value(dtype='int64', id=None), 'name': Value(dtype='string', id=None), 'age': Value(dtype='int64', id=None)}, num_rows: 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJufUpNcoZx-"
      },
      "source": [
        "Let's print second row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liSs-7IEnRiz",
        "outputId": "df045c18-5a00-4f22-ee7a-2e524a0276c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(dataset[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'age': 53, 'id': 1, 'name': 'bob'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbVm7hVLogaG"
      },
      "source": [
        "### Loading from a pandas dataframe\n",
        "\n",
        "Similarly, let's create a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9MwKzbfotxx",
        "outputId": "a009248e-f799-48ab-a55f-3f36b1fe7cdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\"id\": [0, 1, 2, 3], 'name': ['mary', 'bob', 'eve', 'rob'], 'age': [24, 53, 19, 25]})\n",
        "pprint(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  name  age\n",
            "0   0  mary   24\n",
            "1   1   bob   53\n",
            "2   2   eve   19\n",
            "3   3   rob   25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2zGdtvKo7Cp"
      },
      "source": [
        "Let's instantiate a dataset object from the above pandas frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qao06fdxozQn",
        "outputId": "545dfd3c-8012-4471-fa53-e5c8ff4fb5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "pprint(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(features: {'id': Value(dtype='int64', id=None), 'name': Value(dtype='string', id=None), 'age': Value(dtype='int64', id=None)}, num_rows: 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svkfkByepTeo"
      },
      "source": [
        "Let's print second row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmr5Kz5npUf7",
        "outputId": "bb9ed97c-5850-41ee-9bb6-1ea05a8d56ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pprint(dataset[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'age': 53, 'id': 1, 'name': 'bob'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1u6FPCEp5sA"
      },
      "source": [
        "### Using a custom dataset loading script (Bonus)\n",
        "\n",
        "If the provided loading scripts for Hub dataset or for local files are not adapted for our use case, we can also easily write and use our own dataset loading script.\n",
        "\n",
        "We can use a local loading script just by providing its path instead of the usual shortcut name:\n",
        "\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('PATH/TO/MY/LOADING/SCRIPT', data_files='PATH/TO/MY/FILE')\n",
        "```\n",
        "\n",
        "More details on how to create our own dataset generation script on the [Writing a dataset loading script page](https://huggingface.co/docs/datasets/add_dataset.html) and we can also find some inspiration in from the already provided loading scripts on the [GitHub repository](https://github.com/huggingface/datasets/tree/master/datasets).\n",
        "\n",
        "That's it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPl9oy-opWW0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
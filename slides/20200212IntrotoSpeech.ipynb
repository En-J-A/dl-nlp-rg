{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Intro to Speech Processing:**\n",
    "\n",
    "*Yadong Liu & Arian Shamei*\n",
    "\n",
    "**Goal:**  Understand how speech data is discussed, measured, processed, and implemented into deep learning architectures\n",
    "\n",
    "The following tutorial will make use to the Alcohol Language Corpus (Schiel & Barfusser), which contains the sober and intoxicated speech from over 100 individuals. The tutorial will cover the following steps for pre-processing speech data using Parselmouth, Librosa, Python_Speech_Features and other relevant packages. \n",
    "\n",
    "Preprocessing code obtained from the docs page of parselmouth, librosa, and python speech features.\n",
    "Model and dataset building code obtained from Seth Adams: https://github.com/seth814/Audio-Classification\n",
    "\n",
    "**Targets:**\n",
    "1. Evaluating specific metrics from an audio file. \n",
    "2. Pre-emphasis of the audio-file for acoustic feature augmentation\n",
    "3. Generation of Waveforms, Spectrograms, Mel spectrograms, Mel Formant Cepstral Coefficients\n",
    "4. Segmentation of audio data to manageable sizes.\n",
    "5. Incorporate speech data in a deep learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "from librosa import display\n",
    "from matplotlib import cm\n",
    "import pydub\n",
    "from pydub.audio_segment import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from librosa.effects import trim\n",
    "import scipy\n",
    "from scipy.io.wavfile import read\n",
    "import python_speech_features\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "import parselmouth\n",
    "import keras \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,LSTM, MaxPooling2D, LocallyConnected2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "directory = \"C:/Users/User/Desktop/all.ALC.4.cmdi.44539.1573493756/ALC/\"\n",
    "path = \"C:/Users/User/Desktop/clean/\"\n",
    "df = pd.read_table(r\"C:\\Users\\User\\Desktop\\IS2011CHALLENGE\\TRAIN.tbl\", encoding = \"utf8\")\n",
    "df.set_index('file', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is speech?**\n",
    "\n",
    "We begin this tutorial with a brief overview of speech itself, including how speech is described, measured, discussed. \n",
    "\n",
    "    Signals: \n",
    "    Traditionally, phoneticians view speech as an acoustic signal consisting of a complex wave composed of many individual sine waves; the speaker's movements drive air molecules via compression and rarefaction, this in turn propogates energy to other air molecules which eventually influence movement along a sensitive membrane inside the head of surrounding humans. This concept is known as the speech chain. Crucial to the speech chain is the presence of a human listener.\n",
    "\n",
    "    Properties of the air molecules' movement determine how it is perceived. The rate at which an air molecule cycles during compression and rarefaction is referred to as the frequency of a sound, and the degree of pressure underlying the movement is referred to as the amplitude. Frequency is perceived by the listener as pitch, and amplitude is perceived as loudness (intensity). Each sound in speech is a modulation of frequencies and amplitude of the surrounding air, which combined with a time signal, allow for a system of communication.\n",
    "\n",
    "Praat is a highly popular acoustic analysis software which allows sophisticated measurements and analysis of audio signal. Advanced usage of Praat requires familiarity with PraatScript, but parselmouth is a great python interface which allows access to Praat. \n",
    "\n",
    "\n",
    "**Using parselmouth, lets take measurements of a speech along the dimensions we've discussed. Starting by generating a waveform of the first sound file in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snd = parselmouth.Sound(directory + df.index[0][8:])\n",
    "print(np.shape(snd))\n",
    "plt.figure()\n",
    "plt.plot(snd.xs(), snd.values.T)\n",
    "plt.xlim([snd.xmin, snd.xmax])\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A waveform is a direct representation of the signal, with amplitude on the Y-axis and time on the X-axis.\n",
    "\n",
    "When np.shape() is called on the sound file, we get (1, 448938).The second value is the total number of samples in the audio file.\n",
    "\n",
    "Note that the audio file is just over 10 seconds long. What does this tell us about the sampling rate?\n",
    "This likely means we are sampling 44,100 times per second.\n",
    "\n",
    "\n",
    "**Now lets calculate the speaker's mean pitch from the audio file using Parselmouth's to_pitch function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pitch = snd.to_pitch()\n",
    "pitch_values = [f for f in pitch.selected_array['frequency'] if f > 0 and f < 200]\n",
    "np.asarray(pitch_values).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now calculate average intensity across the soundfile**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity = snd.to_intensity()\n",
    "np.asarray(intensity).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The waveform we generated earlier is a visualization with amplitude on the y axis and time on the x axis. Most acoustic analysis is done using *spectrograms*, a three dimensional visualization with frequency on the y axis, time on the x axis, and amplitude represented by darkness.\n",
    "\n",
    "Unlike the waveform above, spectrograms make use of *Fourier transforms* to decompose the signal into its component frequencies.\n",
    "\n",
    "The code below will plot spectograms generated by parselmouth, and also plot pitch and amplitude contours using the measurements we calculated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_spectrogram(spectrogram, dynamic_range=70):\n",
    "    X, Y = spectrogram.x_grid(), spectrogram.y_grid()\n",
    "    sg_db = 10 * np.log10(spectrogram.values)\n",
    "    plt.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap='afmhot')\n",
    "    plt.ylim([spectrogram.ymin, spectrogram.ymax])\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"frequency [Hz]\")\n",
    "    \n",
    "def draw_pitch(pitch):\n",
    "    # Extract selected pitch contour, and\n",
    "    # replace unvoiced samples by NaN to not plot\n",
    "    pitch_values = pitch.selected_array['frequency']\n",
    "    pitch_values[pitch_values==0] = np.nan\n",
    "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=5, color='w')\n",
    "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=2)\n",
    "    plt.grid(False)\n",
    "    plt.ylim(0, pitch.ceiling)\n",
    "    plt.ylabel(\"fundamental frequency [Hz]\")\n",
    "    \n",
    "def draw_intensity(intensity):\n",
    "    plt.plot(intensity.xs(), intensity.values.T, linewidth=3, color='w')\n",
    "    plt.plot(intensity.xs(), intensity.values.T, linewidth=1)\n",
    "    plt.grid(False)\n",
    "    plt.ylim(0)\n",
    "    plt.ylabel(\"intensity [dB]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets generate a spectrogram in Parselmouth, and plot it with the amplitude contour outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity = snd.to_intensity()\n",
    "spectrogram = snd.to_spectrogram()\n",
    "plt.figure()\n",
    "draw_spectrogram(spectrogram)\n",
    "plt.twinx()\n",
    "draw_intensity(intensity)\n",
    "plt.xlim([snd.xmin, snd.xmax])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that the spectrogram above is somewhat muddy and difficult to interpret. This is known as a wide-band spectrogram. Wide-band spectrograms have better resolution for time than frequency. We can generate a narrow band spectrogram instead by increasing the window length.\n",
    "\n",
    "We can also add pre-emphasis to the audio to enhance frequencies used in speech. The code below plots a pre-emphasized narrow-band spectrogram, and outlines the pitch contour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_emphasized_snd = snd.copy()\n",
    "pre_emphasized_snd.pre_emphasize()\n",
    "spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
    "plt.figure()\n",
    "draw_spectrogram(spectrogram)\n",
    "plt.twinx()\n",
    "draw_pitch(pitch)\n",
    "plt.xlim([snd.xmin, snd.xmax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pre-emphasis enhanced the frequencies used in speech, while ignoring the non-speech noise at the very end of the recording. \n",
    "\n",
    "The audio we have been plotting is ten seconds long. Using parselmouth, we can extract any specific portion of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a snippet of the recording\n",
    "snd_part = snd.extract_part(from_time=1.0,to_time=3.0, preserve_times=False)\n",
    "pre_emphasized_snd = snd_part.copy()\n",
    "#pre-emphasize the snippet\n",
    "pre_emphasized_snd.pre_emphasize()\n",
    "#plot pre-emphasized snippet spectrogram\n",
    "spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
    "plt.figure()\n",
    "draw_spectrogram(spectrogram)\n",
    "plt.xlim([snd_part.xmin, snd_part.xmax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spectrogram above, the bright horizontal lines are *formants* which are the spectral components of vowels. \n",
    "\n",
    "The grainy bars of *broad-spectrum noise* which appear between formants are the spectral components of consonants. \n",
    "\n",
    "Another variation of spectrogram is the *mel spectrogram*. Mel spectrograms constrain spectrograms along the *mel scale*. \n",
    "The mel scale is a non-linear transformation of a frequency range to represent it according to human perception.\n",
    "\n",
    "Humans are very sensitive to different frequencies in the lower ranges (100Hz vs 300 Hz), but this difference is lost at greater values (3000Hz vs 3300Hz). The mel scale accurately represents this change in resolution. Incorporating the mel scale into deep learning models assists them in learning human concepts. \n",
    "\n",
    "\n",
    "Using another audio processing toolkit, *Librosa*, let's generate a mel spectrogram for the same audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(directory + df.index[0][8:], sr = 44100)\n",
    "S = librosa.feature.melspectrogram(y, sr=sr, n_mels=256, n_fft = 2048, hop_length=32,\n",
    "                                   fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB,sr=sr,\n",
    "                             fmax=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librosa also has a handry trim function, which will remove leading and trailing silence audio files. \n",
    "\n",
    "Lets use this trim function on the dataset to clean the audio files in preparation for our model. Simultaneously, to speed up data processing, we are going to downsample the data from 44.1KHz to 22KHz. The cleaned training files can be found in the 'clean' folder, and the dev/test set files can be found in the 'cleantest' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(directory + df.index[0][8:])\n",
    "yt,index = librosa.effects.trim(y, top_db=25)\n",
    "S = librosa.feature.melspectrogram(y, sr=22050, n_mels=256, n_fft = 2048, hop_length=32,\n",
    "                                   fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB,sr=sr,\n",
    "                             fmax=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of mel-based visualization widely employed in audio DL models is the Mel Frequency Cepstral Coefficient (MFCC):\n",
    "MFCCs are a series of short-term power spectrums over time, where frequency components are sorted into mel-banks on the y-axis across time on the x-axis. \n",
    "\n",
    "Both Librosa and python_speech_features allows very easy MFCC generation. We will use python_speech_features to plot an MFCC across one second of audio from a file in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, sig = scipy.io.wavfile.read(directory+df.index[1][8:])\n",
    "mfcc_feat = mfcc(sig[rate*2:rate*3],rate, numcep=13, nfilt=26, nfft=1103)\n",
    "\n",
    "ig, ax = plt.subplots()\n",
    "mfcc_data= np.swapaxes(mfcc_feat, 0 ,1)\n",
    "cax = ax.imshow(mfcc_data, interpolation='nearest', cmap=cm.hot, origin='lower', aspect='auto')\n",
    "ax.set_title('MFCC')\n",
    "#Showing mfcc_data\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train our model using MFCCs generated over one second of audio. To generate the correct amount of samples, we are going to need to know the length of each file, and the distribution of data over each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in df.index:\n",
    "    rate, signal = scipy.io.wavfile.read(path+f[16:])\n",
    "    df.at[f, 'length'] = signal.shape[0]/rate\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(np.unique(df.state))\n",
    "class_dist = df.groupby(['state'])['length'].mean()\n",
    "class_sum = df.groupby(['state'])['length'].sum()\n",
    "class_avg_len = df.groupby(['state'])['length'].mean()\n",
    "prob_dist = class_dist / class_dist.sum()\n",
    "n_samples = 2*int(df['length'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "I    0.344025\n",
       "S    0.655975\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist = class_dist / class_dist.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "I    10.021556\n",
       "S     9.662736\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the length of our files, we know how many one second samples to take from the data. \n",
    "\n",
    "Below is a function that will randomly sample one second MFCCs from audio files to build our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rand_feat():\n",
    "    X=[]\n",
    "    y=[]\n",
    "    #for data normalization we will create a rolling log of minimum and maximum values\n",
    "    _min, _max = float('inf'), -float('inf')\n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        #sample from each class randomly according to the calculated probability distribution:\n",
    "        rand_class= np.random.choice(class_dist.index, p=prob_dist)\n",
    "        file = np.random.choice(df.index)\n",
    "        rate, wav  = scipy.io.wavfile.read(path+file[16:])\n",
    "        label = df.at[file, 'state']\n",
    "        #Ensures that each file is long enough for a one second sample (at least 2 seconds long)\n",
    "        if wav.shape[0] > rate*2:\n",
    "            #choose a random timepoint in the audio file to draw the one-second sample\n",
    "            rand_index = np.random.randint(0, wav.shape[0]-config.step)\n",
    "            sample = wav[rand_index:rand_index+config.step]\n",
    "            #generate MFCC from the random sample\n",
    "            X_sample = mfcc(sample, rate, \n",
    "                          numcep=config.nfeat, \n",
    "                          nfilt =  config.nfilt,\n",
    "                          nfft = config.nfft).T\n",
    "            #update rolling normalization window\n",
    "            _min = min(np.amin(X_sample), _min)\n",
    "            _max = max(np.amax(X_sample), _max)\n",
    "            #append MFCC vector\n",
    "            X.append(X_sample if config.mode == 'conv' else X_sample.T)           \n",
    "            y.append(classes.index(label))\n",
    "        else:\n",
    "            continue\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = (X - _min) / (_max - _min)\n",
    "    if config.mode == 'conv':\n",
    "        X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1 )\n",
    "    y= to_categorical(y, num_classes=2)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built our random feature set for the audio data, we define our model. As a starting poiny, we have chosen a two layer 2DCNN, with pooling at each layer and  dropout on the final layer. The optimizer adadelta was used as in past experiments it worked well for speaker independent data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape,use_bias=True,kernel_initializer='normal', padding =\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())  #\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    adadelta = optimizers.adadelta()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adadelta,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, mode='conv', nfilt=26, nfeat=13, nfft=2048, rate=22050, n_mels=256, hoplength=32,):\n",
    "        self.mode = mode\n",
    "        self.nfilt = nfilt\n",
    "        self.nfeat = nfeat\n",
    "        self.rate = rate\n",
    "        self.nfft= nfft\n",
    "        self.step = int(rate)\n",
    "        self.nmels = n_mels\n",
    "        self.hoplength = hoplength\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(mode='conv')\n",
    "if config.mode == 'conv':\n",
    "    X,y = build_rand_feat()\n",
    "    y_flat = np.argmax(y, axis=1)\n",
    "    input_shape = (X.shape[1], X.shape[2],1)\n",
    "    model = get_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90932 samples, validate on 49628 samples\n",
      "Epoch 1/100\n",
      " - 52s - loss: 0.6444 - acc: 0.6527 - val_loss: 0.7829 - val_acc: 0.4969\n",
      "Epoch 2/100\n",
      " - 39s - loss: 0.6337 - acc: 0.6574 - val_loss: 0.7371 - val_acc: 0.5111\n",
      "Epoch 3/100\n",
      " - 39s - loss: 0.6275 - acc: 0.6644 - val_loss: 0.7263 - val_acc: 0.5152\n",
      "Epoch 4/100\n",
      " - 39s - loss: 0.6239 - acc: 0.6679 - val_loss: 0.7426 - val_acc: 0.5152\n",
      "Epoch 5/100\n",
      " - 39s - loss: 0.6213 - acc: 0.6693 - val_loss: 0.7481 - val_acc: 0.5215\n",
      "Epoch 6/100\n",
      " - 39s - loss: 0.6170 - acc: 0.6731 - val_loss: 0.7440 - val_acc: 0.5325\n",
      "Epoch 7/100\n",
      " - 39s - loss: 0.6132 - acc: 0.6750 - val_loss: 0.7625 - val_acc: 0.5241\n",
      "Epoch 8/100\n",
      " - 39s - loss: 0.6100 - acc: 0.6772 - val_loss: 0.7524 - val_acc: 0.5347\n",
      "Epoch 9/100\n",
      " - 39s - loss: 0.6082 - acc: 0.6799 - val_loss: 0.7414 - val_acc: 0.5404\n",
      "Epoch 10/100\n",
      " - 39s - loss: 0.6058 - acc: 0.6808 - val_loss: 0.7719 - val_acc: 0.5275\n",
      "Epoch 11/100\n",
      " - 39s - loss: 0.6034 - acc: 0.6815 - val_loss: 0.7702 - val_acc: 0.5339\n",
      "Epoch 12/100\n",
      " - 39s - loss: 0.6015 - acc: 0.6830 - val_loss: 0.7300 - val_acc: 0.5422\n",
      "Epoch 13/100\n",
      " - 39s - loss: 0.5984 - acc: 0.6858 - val_loss: 0.7695 - val_acc: 0.5355\n",
      "Epoch 14/100\n",
      " - 39s - loss: 0.5964 - acc: 0.6867 - val_loss: 0.7574 - val_acc: 0.5357\n",
      "Epoch 15/100\n",
      " - 39s - loss: 0.5941 - acc: 0.6887 - val_loss: 0.7483 - val_acc: 0.5459\n",
      "Epoch 16/100\n",
      " - 39s - loss: 0.5922 - acc: 0.6908 - val_loss: 0.7467 - val_acc: 0.5428\n",
      "Epoch 17/100\n",
      " - 39s - loss: 0.5888 - acc: 0.6935 - val_loss: 0.7430 - val_acc: 0.5446\n",
      "Epoch 18/100\n",
      " - 39s - loss: 0.5861 - acc: 0.6942 - val_loss: 0.7360 - val_acc: 0.5532\n",
      "Epoch 19/100\n",
      " - 39s - loss: 0.5828 - acc: 0.6978 - val_loss: 0.8248 - val_acc: 0.5313\n",
      "Epoch 20/100\n",
      " - 39s - loss: 0.5794 - acc: 0.6993 - val_loss: 0.8142 - val_acc: 0.5368\n",
      "Epoch 21/100\n",
      " - 39s - loss: 0.5773 - acc: 0.7011 - val_loss: 0.7583 - val_acc: 0.5475\n",
      "Epoch 22/100\n",
      " - 39s - loss: 0.5748 - acc: 0.7050 - val_loss: 0.7656 - val_acc: 0.5349\n",
      "Epoch 23/100\n",
      " - 39s - loss: 0.5729 - acc: 0.7058 - val_loss: 0.8052 - val_acc: 0.5498\n",
      "Epoch 24/100\n",
      " - 39s - loss: 0.5707 - acc: 0.7061 - val_loss: 0.7627 - val_acc: 0.5449\n",
      "Epoch 25/100\n",
      " - 39s - loss: 0.5670 - acc: 0.7092 - val_loss: 0.8468 - val_acc: 0.5358\n",
      "Epoch 26/100\n",
      " - 39s - loss: 0.5653 - acc: 0.7109 - val_loss: 0.7778 - val_acc: 0.5590\n",
      "Epoch 27/100\n",
      " - 39s - loss: 0.5630 - acc: 0.7133 - val_loss: 0.7603 - val_acc: 0.5437\n",
      "Epoch 28/100\n",
      " - 39s - loss: 0.5609 - acc: 0.7133 - val_loss: 0.8671 - val_acc: 0.5378\n",
      "Epoch 29/100\n",
      " - 39s - loss: 0.5598 - acc: 0.7159 - val_loss: 0.7740 - val_acc: 0.5478\n",
      "Epoch 30/100\n",
      " - 39s - loss: 0.5574 - acc: 0.7178 - val_loss: 0.8788 - val_acc: 0.5231\n",
      "Epoch 31/100\n",
      " - 39s - loss: 0.5580 - acc: 0.7164 - val_loss: 0.8463 - val_acc: 0.5342\n",
      "Epoch 32/100\n",
      " - 39s - loss: 0.5550 - acc: 0.7200 - val_loss: 0.8548 - val_acc: 0.5382\n",
      "Epoch 33/100\n",
      " - 39s - loss: 0.5535 - acc: 0.7199 - val_loss: 0.7777 - val_acc: 0.5578\n",
      "Epoch 34/100\n",
      " - 39s - loss: 0.5526 - acc: 0.7209 - val_loss: 0.8366 - val_acc: 0.5400\n",
      "Epoch 35/100\n",
      " - 39s - loss: 0.5517 - acc: 0.7214 - val_loss: 0.8833 - val_acc: 0.5385\n",
      "Epoch 36/100\n",
      " - 39s - loss: 0.5496 - acc: 0.7229 - val_loss: 0.8793 - val_acc: 0.5513\n",
      "Epoch 37/100\n",
      " - 39s - loss: 0.5486 - acc: 0.7256 - val_loss: 0.8607 - val_acc: 0.5448\n",
      "Epoch 38/100\n",
      " - 39s - loss: 0.5483 - acc: 0.7239 - val_loss: 0.9292 - val_acc: 0.5477\n",
      "Epoch 39/100\n",
      " - 39s - loss: 0.5464 - acc: 0.7267 - val_loss: 0.8289 - val_acc: 0.5529\n",
      "Epoch 40/100\n",
      " - 39s - loss: 0.5449 - acc: 0.7268 - val_loss: 0.9534 - val_acc: 0.5412\n",
      "Epoch 41/100\n",
      " - 39s - loss: 0.5448 - acc: 0.7266 - val_loss: 0.9135 - val_acc: 0.5502\n",
      "Epoch 42/100\n",
      " - 39s - loss: 0.5439 - acc: 0.7283 - val_loss: 0.9517 - val_acc: 0.5471\n",
      "Epoch 43/100\n",
      " - 40s - loss: 0.5399 - acc: 0.7294 - val_loss: 0.8189 - val_acc: 0.5519\n",
      "Epoch 44/100\n",
      " - 40s - loss: 0.5399 - acc: 0.7297 - val_loss: 0.8411 - val_acc: 0.5481\n",
      "Epoch 45/100\n",
      " - 41s - loss: 0.5397 - acc: 0.7308 - val_loss: 0.8665 - val_acc: 0.5513\n",
      "Epoch 46/100\n",
      " - 39s - loss: 0.5390 - acc: 0.7318 - val_loss: 0.8520 - val_acc: 0.5583\n",
      "Epoch 47/100\n",
      " - 40s - loss: 0.5357 - acc: 0.7334 - val_loss: 0.8578 - val_acc: 0.5363\n",
      "Epoch 48/100\n",
      " - 40s - loss: 0.5363 - acc: 0.7332 - val_loss: 0.8987 - val_acc: 0.5532\n",
      "Epoch 49/100\n",
      " - 40s - loss: 0.5334 - acc: 0.7351 - val_loss: 0.8457 - val_acc: 0.5498\n",
      "Epoch 50/100\n",
      " - 43s - loss: 0.5329 - acc: 0.7347 - val_loss: 0.9303 - val_acc: 0.5516\n",
      "Epoch 51/100\n",
      " - 41s - loss: 0.5339 - acc: 0.7349 - val_loss: 0.8761 - val_acc: 0.5465\n",
      "Epoch 52/100\n",
      " - 41s - loss: 0.5330 - acc: 0.7367 - val_loss: 0.9710 - val_acc: 0.5394\n",
      "Epoch 53/100\n",
      " - 42s - loss: 0.5310 - acc: 0.7361 - val_loss: 0.9511 - val_acc: 0.5457\n",
      "Epoch 54/100\n",
      " - 41s - loss: 0.5283 - acc: 0.7394 - val_loss: 0.8905 - val_acc: 0.5534\n",
      "Epoch 55/100\n",
      " - 41s - loss: 0.5280 - acc: 0.7388 - val_loss: 0.9512 - val_acc: 0.5444\n",
      "Epoch 56/100\n",
      " - 41s - loss: 0.5273 - acc: 0.7403 - val_loss: 0.9456 - val_acc: 0.5433\n",
      "Epoch 57/100\n",
      " - 41s - loss: 0.5250 - acc: 0.7429 - val_loss: 0.9185 - val_acc: 0.5476\n",
      "Epoch 58/100\n",
      " - 41s - loss: 0.5244 - acc: 0.7423 - val_loss: 1.1082 - val_acc: 0.5414\n",
      "Epoch 59/100\n",
      " - 41s - loss: 0.5230 - acc: 0.7431 - val_loss: 0.9368 - val_acc: 0.5485\n",
      "Epoch 60/100\n",
      " - 40s - loss: 0.5224 - acc: 0.7447 - val_loss: 0.9869 - val_acc: 0.5512\n",
      "Epoch 61/100\n",
      " - 42s - loss: 0.5223 - acc: 0.7444 - val_loss: 1.0147 - val_acc: 0.5532\n",
      "Epoch 62/100\n",
      " - 41s - loss: 0.5212 - acc: 0.7454 - val_loss: 0.8276 - val_acc: 0.5468\n",
      "Epoch 63/100\n",
      " - 41s - loss: 0.5187 - acc: 0.7469 - val_loss: 0.9690 - val_acc: 0.5496\n",
      "Epoch 64/100\n",
      " - 41s - loss: 0.5204 - acc: 0.7460 - val_loss: 1.0091 - val_acc: 0.5502\n",
      "Epoch 65/100\n",
      " - 51s - loss: 0.5184 - acc: 0.7472 - val_loss: 0.9202 - val_acc: 0.5432\n",
      "Epoch 66/100\n",
      " - 43s - loss: 0.5182 - acc: 0.7472 - val_loss: 0.9375 - val_acc: 0.5575\n",
      "Epoch 67/100\n",
      " - 43s - loss: 0.5172 - acc: 0.7494 - val_loss: 0.9927 - val_acc: 0.5420\n",
      "Epoch 68/100\n",
      " - 42s - loss: 0.5162 - acc: 0.7494 - val_loss: 0.8998 - val_acc: 0.5556\n",
      "Epoch 69/100\n",
      " - 43s - loss: 0.5153 - acc: 0.7489 - val_loss: 1.0025 - val_acc: 0.5536\n",
      "Epoch 70/100\n",
      " - 45s - loss: 0.5146 - acc: 0.7489 - val_loss: 1.0514 - val_acc: 0.5511\n",
      "Epoch 71/100\n",
      " - 43s - loss: 0.5130 - acc: 0.7508 - val_loss: 0.9204 - val_acc: 0.5512\n",
      "Epoch 72/100\n",
      " - 44s - loss: 0.5137 - acc: 0.7513 - val_loss: 1.0099 - val_acc: 0.5456\n",
      "Epoch 73/100\n",
      " - 43s - loss: 0.5133 - acc: 0.7518 - val_loss: 1.0685 - val_acc: 0.5393\n",
      "Epoch 74/100\n",
      " - 42s - loss: 0.5139 - acc: 0.7519 - val_loss: 1.0104 - val_acc: 0.5568\n",
      "Epoch 75/100\n",
      " - 45s - loss: 0.5119 - acc: 0.7519 - val_loss: 1.0911 - val_acc: 0.5425\n",
      "Epoch 76/100\n",
      " - 43s - loss: 0.5109 - acc: 0.7530 - val_loss: 0.9388 - val_acc: 0.5422\n",
      "Epoch 77/100\n",
      " - 43s - loss: 0.5088 - acc: 0.7549 - val_loss: 0.9458 - val_acc: 0.5525\n",
      "Epoch 78/100\n",
      " - 41s - loss: 0.5098 - acc: 0.7535 - val_loss: 0.9633 - val_acc: 0.5495\n",
      "Epoch 79/100\n",
      " - 40s - loss: 0.5091 - acc: 0.7545 - val_loss: 1.0024 - val_acc: 0.5454\n",
      "Epoch 80/100\n",
      " - 43s - loss: 0.5080 - acc: 0.7561 - val_loss: 0.9235 - val_acc: 0.5520\n",
      "Epoch 81/100\n",
      " - 42s - loss: 0.5078 - acc: 0.7579 - val_loss: 0.9965 - val_acc: 0.5530\n",
      "Epoch 82/100\n",
      " - 45s - loss: 0.5060 - acc: 0.7557 - val_loss: 0.9576 - val_acc: 0.5471\n",
      "Epoch 83/100\n",
      " - 43s - loss: 0.5046 - acc: 0.7580 - val_loss: 1.0131 - val_acc: 0.5473\n",
      "Epoch 84/100\n",
      " - 44s - loss: 0.5074 - acc: 0.7575 - val_loss: 1.0670 - val_acc: 0.5485\n",
      "Epoch 85/100\n",
      " - 44s - loss: 0.5060 - acc: 0.7582 - val_loss: 1.0129 - val_acc: 0.5490\n",
      "Epoch 86/100\n",
      " - 43s - loss: 0.5049 - acc: 0.7589 - val_loss: 1.1547 - val_acc: 0.5529\n",
      "Epoch 87/100\n",
      " - 47s - loss: 0.5066 - acc: 0.7571 - val_loss: 0.9209 - val_acc: 0.5497\n",
      "Epoch 88/100\n",
      " - 45s - loss: 0.5034 - acc: 0.7595 - val_loss: 1.0911 - val_acc: 0.5457\n",
      "Epoch 89/100\n",
      " - 44s - loss: 0.5062 - acc: 0.7586 - val_loss: 0.9524 - val_acc: 0.5502\n",
      "Epoch 90/100\n",
      " - 45s - loss: 0.5042 - acc: 0.7580 - val_loss: 0.9818 - val_acc: 0.5463\n",
      "Epoch 91/100\n",
      " - 42s - loss: 0.5051 - acc: 0.7587 - val_loss: 1.1765 - val_acc: 0.5536\n",
      "Epoch 92/100\n",
      " - 41s - loss: 0.5034 - acc: 0.7596 - val_loss: 1.1574 - val_acc: 0.5529\n",
      "Epoch 93/100\n",
      " - 41s - loss: 0.5041 - acc: 0.7595 - val_loss: 1.0873 - val_acc: 0.5551\n",
      "Epoch 94/100\n",
      " - 41s - loss: 0.5031 - acc: 0.7589 - val_loss: 1.0494 - val_acc: 0.5500\n",
      "Epoch 95/100\n",
      " - 40s - loss: 0.5025 - acc: 0.7603 - val_loss: 1.0907 - val_acc: 0.5507\n",
      "Epoch 96/100\n",
      " - 41s - loss: 0.5019 - acc: 0.7627 - val_loss: 1.0571 - val_acc: 0.5438\n",
      "Epoch 97/100\n",
      " - 41s - loss: 0.5038 - acc: 0.7602 - val_loss: 1.0002 - val_acc: 0.5431\n",
      "Epoch 98/100\n",
      " - 41s - loss: 0.5030 - acc: 0.7608 - val_loss: 1.3916 - val_acc: 0.5397\n",
      "Epoch 99/100\n",
      " - 42s - loss: 0.5019 - acc: 0.7612 - val_loss: 1.0230 - val_acc: 0.5527\n",
      "Epoch 100/100\n",
      " - 40s - loss: 0.5006 - acc: 0.7614 - val_loss: 1.1011 - val_acc: 0.5518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18893032e48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, batch_size =32, \n",
    "         shuffle=True,validation_data=(X_test,y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test2_feat():\n",
    "    X_test2=[]\n",
    "    y_test2=[]\n",
    "    _min, _max = float('inf'), -float('inf')\n",
    "    for _ in tqdm(range(t2_samples)):\n",
    "        rand_class= np.random.choice(t2_class_dist.index, p=t2_prob_dist)\n",
    "        file = np.random.choice(df3.index)\n",
    "        rate, wav  = scipy.io.wavfile.read(directory+file[16:])\n",
    "        label = df3.at[file, 'state']\n",
    "        if wav.shape[0] > rate*2:\n",
    "            rand_index = np.random.randint(0, wav.shape[0]-config.step)\n",
    "            sample = wav[rand_index:rand_index+config.step]\n",
    "            X_sample = mfcc(sample, rate, \n",
    "                          numcep=config.nfeat, \n",
    "                          nfilt =  config.nfilt,\n",
    "                          nfft = config.nfft).T\n",
    "            _min = min(np.amin(X_sample), _min)\n",
    "            _max = max(np.amax(X_sample), _max)\n",
    "            X_test2.append(X_sample if config.mode == 'conv' else X_sample.T)           \n",
    "            y_test2.append(classes.index(label))\n",
    "        else:\n",
    "            continue\n",
    "    X_test2, y_test2 = np.array(X_test2), np.array(y_test2)\n",
    "    X_test2 = (X_test2 - _min) / (_max - _min)\n",
    "    if config.mode == 'conv':\n",
    "        X_test2 = X_test2.reshape(X_test2.shape[0], X_test2.shape[1], X_test2.shape[2], 1 )\n",
    "    elif config.mode== 'time':\n",
    "        X_test2 = X_test2.reshape(X_test2.shape[0], X_test2.shape[1], X_test2.shape[2] )\n",
    "    y_test2= to_categorical(y_test2, num_classes=2)\n",
    "    return X_test2, y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
